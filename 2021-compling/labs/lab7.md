# Encoder-Decoder Lab

For this lab, we will again use some Kerasâ€™s example starter code for LSTM-based encoder-decoder models here: https://keras.io/examples/nlp/lstm_seq2seq/.  The purpose of this lab if to give students some intuition some experience training a simple neural machine trnaslation model

Read the tutorial and run the code, either on a Haverford GPU machine or Google Collab.  Recall that the encoder LSTM (or GRU) creates a contextualized representation of the input tokens.  The following line returns the hidden state and a cell state, returned by the following line:

```python
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]
```

The variable `state_h` holds the final hidden state, after all characters have been read.  The initial hidden state of the decoder is then set to the final hidden state of the encoder.

```python
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
```



Then the model chooses the next character to predict with a softmax over all possible characters. 

```
decoder_dense = keras.layers.Dense(num_decoder_tokens, activation="softmax")
```

The task for this lab is simple:  Collaborate (in person and/or on Piazza) with other students to run the translation system for various numbers of epochs and other parameters and make note of the kinds of translations produced.  You will likely find that the translation systems struggles significantly and isn't entirely consistent in what it learns. Submit to Moodle qualitative observations of what you found and who you collaborated with.

